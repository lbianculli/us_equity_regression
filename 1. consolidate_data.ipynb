{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO: \n",
    "- align methodology (col names etc etc) exactly to new data gathering process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sqlalchemy import create_engine  \n",
    "import datetime \n",
    "import yfinance as yf\n",
    "import warnings\n",
    "# import norgatedata\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# load metadata and relevant data\n",
    "sharadar_sectors = pd.read_csv(\"C:/Users/lbianculli/backup/quant_analysis/dev/data/maps/sharadar_meta2.csv\")\n",
    "price_data = pd.read_csv(\"C:/Users/lbianculli/dev/us_equities/norgate/nogate_pricing.csv\").drop([\"Unnamed: 0\"], axis=1)\n",
    "all_macro_data = pd.read_csv(\"C:/Users/lbianculli/dev/us_equities/macro/macro_data.csv\").drop([\"Unnamed: 0\"], axis=1)\n",
    "\n",
    "# fundamental data\n",
    "all_fundamental_data = pd.read_csv(\"C:/Users/lbianculli/dev/us_equities/ML_categorization/quandl_fundamentals.csv\")\n",
    "all_fundamental_data = all_fundamental_data.loc[all_fundamental_data[\"dimension\"] == \"MRQ\"]\n",
    "\n",
    "for c in all_fundamental_data.columns:\n",
    "    try:\n",
    "        all_fundamental_data[c] = all_fundamental_data[c].astype(float)\n",
    "    except Exception as e:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get nearest business day so we can merge later\n",
    "from pandas.tseries.offsets import BDay\n",
    "\n",
    "price_data[\"date_dt\"] = pd.to_datetime(price_data[\"date_dt\"], format='%Y-%m-%d')\n",
    "all_fundamental_data[\"date_dt\"] = pd.to_datetime(all_fundamental_data[\"calendar_date\"], format='%Y-%m-%d')\n",
    "all_fundamental_data[\"concat\"] = all_fundamental_data[\"ticker_symbol\"] + \"-\" + all_fundamental_data[\"date_dt\"].dt.strftime(date_format=\"%Y-%m-%d\")\n",
    "all_fundamental_data[\"ticker_symbol\"] = all_fundamental_data[\"ticker_symbol\"].str.replace(\".\", \"-\")  # YF idiosyncracy\n",
    "\n",
    "# get largest companies according to average market cap\n",
    "n_largest = 1000\n",
    "grouped_data = all_fundamental_data.groupby(\"ticker_symbol\").mean()\n",
    "largest_company_data = grouped_data.sort_values(\"market_capitalization\", ascending=False).head(n_largest)\n",
    "largest_companies = largest_company_data.index.unique()\n",
    "largest_companies = [c.replace(\"-\", \".\") for c in largest_companies]\n",
    "\n",
    "# get fundamentals of all largest companies. Aggregation above would have dropped some columns\n",
    "largest_company_data = all_fundamental_data.loc[all_fundamental_data[\"ticker_symbol\"].isin(largest_companies)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tp_tag(row):\n",
    "    \"\"\"\n",
    "    0: rebound\n",
    "    1: bull\n",
    "    2: bear\n",
    "    3: contraction\n",
    "    \"\"\"\n",
    "    if (row[\"fast_tag\"] > 0) & (row[\"slow_tag\"] < 0):\n",
    "        return 0 \n",
    "    \n",
    "    if (row[\"fast_tag\"] > 0) & (row[\"slow_tag\"] > 0):\n",
    "        return 1 \n",
    "    \n",
    "    if (row[\"fast_tag\"] < 0) & (row[\"slow_tag\"] < 0):\n",
    "        return 2 \n",
    "    \n",
    "    if (row[\"fast_tag\"] < 0) & (row[\"slow_tag\"] > 0):\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- BRK.B: No data found, symbol may be delisted\n",
      "- TWX: No data found for this date range, symbol may be delisted\n",
      "- MOB: No data found for this date range, symbol may be delisted\n",
      "- TWX: No data found for this date range, symbol may be delisted\n",
      "- WYE: No data found for this date range, symbol may be delisted\n",
      "- TFCF: No data found, symbol may be delisted\n",
      "- WLA: No data found for this date range, symbol may be delisted\n",
      "- TYC: No data found for this date range, symbol may be delisted\n",
      "- CPQ: No data found for this date range, symbol may be delisted\n",
      "- KRFT: No data found for this date range, symbol may be delisted\n",
      "- USW: No data found for this date range, symbol may be delisted\n",
      "- JAVA: Data doesn't exist for startDate = 1535785200, endDate = 1633071600\n",
      "- CELG: No data found, symbol may be delisted\n",
      "- FBF: No data found for this date range, symbol may be delisted\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Add in the \n",
    "equity_data = {}\n",
    "\n",
    "macro_columns = ['unemp_ma_diff', 'confidence', 'MICH', 'bus_credit_change', 'con_credit_change', \n",
    "                 'pce_change', 'Adj Close', 'vix_change', 'inverted']\n",
    "\n",
    "# # norgate params\n",
    "# price_adjust = norgatedata.StockPriceAdjustmentType.TOTALRETURN \n",
    "# padding_setting = norgatedata.PaddingType.NONE   \n",
    "# start_date = '2020-10-01'  # should change to look up off dataset\n",
    "# ts_format = 'pandas-dataframe'\n",
    "\n",
    "for ticker in largest_companies:\n",
    "    company_data = all_fundamental_data.loc[all_fundamental_data[\"ticker_symbol\"] == ticker]\n",
    "\n",
    "    if ticker[-1] == \"1\":\n",
    "        ticker = ticker[:-1]\n",
    "        \n",
    "    if ticker not in price_data[\"ticker\"].unique():\n",
    "        ticker_obj = yf.Ticker(ticker)\n",
    "        company_price_data = ticker_obj.history(start='2018-09-01', end='2021-10-01').reset_index()\n",
    "        \n",
    "        try:\n",
    "            company_price_data.columns = [\"date_dt\", \"open\", \"high\", \"low\", \"close\", \"volume\", \"div\", \"splits\"]\n",
    "        except Exception as e:  # occurs if no data\n",
    "            company_price_data.columns = [\"date_dt\", \"open\", \"high\", \"low\", \"close1\", \"close\", \"volume\"]\n",
    "        \n",
    "        company_price_data[\"ticker\"] = ticker\n",
    "        \n",
    "    else:\n",
    "        company_price_data = price_data.loc[price_data[\"ticker\"] == ticker]\n",
    "\n",
    "    if company_price_data.shape[0] > 0:\n",
    "        ### Price Data\n",
    "        # first, get updated data that we can use to merge with historical data\n",
    "        try:\n",
    "            # get columns to align with historical data\n",
    "            company_price_data[\"date\"] = company_price_data[\"date_dt\"].dt.strftime(date_format=\"%Y-%m-%d\")\n",
    "            company_price_data[\"year\"] = company_price_data[\"date_dt\"].dt.year\n",
    "            company_price_data[\"month\"] = company_price_data[\"date_dt\"].dt.month\n",
    "            company_price_data[\"ticker\"] = ticker\n",
    "            company_price_data[\"period\"] = company_price_data[\"year\"].astype(str) + \"-\" + company_price_data[\"month\"].astype(str)\n",
    "            company_price_data[\"id\"] = company_price_data[\"ticker\"] + \"-\" + company_price_data[\"period\"]\n",
    "\n",
    "            # merge\n",
    "            company_price_data = pd.concat([company_price_data, company_price_data])\n",
    "\n",
    "        except ValueError as e:\n",
    "    #         print(f\"No new price data available for {ticker}\")\n",
    "            pass\n",
    "\n",
    "        # get month and year end date for calculating returns\n",
    "        company_price_data[\"day\"] = company_price_data[\"date_dt\"].dt.day\n",
    "        company_price_data[\"day_tag\"] = company_price_data[\"day\"].shift(-1) - company_price_data[\"day\"]\n",
    "        company_price_data[\"day_tag\"] = company_price_data[\"day_tag\"].replace(np.nan, -1)  # in case we get between financial release and end of month\n",
    "\n",
    "        # get returns and volatility\n",
    "        company_price_data[\"rets\"] = company_price_data[\"close\"] / company_price_data[\"close\"].shift(1) - 1\n",
    "        company_price_data[\"log_rets\"] = np.log(1 + company_price_data[\"rets\"])\n",
    "        company_price_data[\"volatility\"] = company_price_data[\"log_rets\"].rolling(252).std()\n",
    "        company_price_data[\"1mo_rets\"] = company_price_data[\"close\"] / company_price_data[\"close\"].shift(21) - 1\n",
    "        company_price_data[\"1yr_rets\"] = company_price_data[\"close\"] / company_price_data[\"close\"].shift(252) - 1\n",
    "\n",
    "        # compute log and forward returns\n",
    "        company_price_data[\"1mo_log_rets\"] = np.log(1 + company_price_data[\"1mo_rets\"])\n",
    "        company_price_data[\"1yr_log_rets\"] = np.log(1 + company_price_data[\"1yr_rets\"])\n",
    "\n",
    "        company_price_data[\"1mo_fwd_rets\"] = company_price_data[\"close\"].shift(-21) / company_price_data[\"close\"] - 1\n",
    "        company_price_data[\"1mo_fwd_log_rets\"] = np.log(1 + company_price_data[\"1mo_fwd_rets\"])\n",
    "\n",
    "        # limit to month-end data to align with fundaments\n",
    "        monthly_data = company_price_data.loc[company_price_data[\"day_tag\"] < 0]\n",
    "\n",
    "        # get tags for momentum windows\n",
    "        try:\n",
    "            monthly_data[\"fast_tag\"] = np.where(monthly_data[\"1mo_rets\"] < 0, -1, 1) \n",
    "            monthly_data[\"slow_tag\"] = np.where(monthly_data[\"1yr_rets\"] < 0, -1, 1)\n",
    "            monthly_data[[\"1mo_rets\", \"1yr_rets\", \"slow_tag\", \"fast_tag\"]].head()\n",
    "            monthly_data[\"tp_tag\"] = monthly_data.apply(lambda x: tp_tag(x), axis=1)\n",
    "\n",
    "            # get turning point, defined as disagreement between slow and fast signals (does this make sense??)\n",
    "            monthly_data[\"tp\"] = np.where(monthly_data[\"slow_tag\"] != monthly_data[\"fast_tag\"], 1, 0)\n",
    "            monthly_data[\"tp_last_yr\"] = monthly_data[\"tp\"].rolling(12).sum()\n",
    "\n",
    "        except Exception as e:\n",
    "            pass  # no data\n",
    "\n",
    "        ### Fundamental data\n",
    "        # and get other fundamental data as well\n",
    "        company_data = company_data.sort_values(\"calendar_date\")\n",
    "        \n",
    "        # create standard deviation (variability calcs)\n",
    "        company_data[\"fcf_std\"] = company_data[\"free_cash_flow_per_share\"].iloc[-4:].std() \n",
    "        company_data[\"capex_std\"] = company_data[\"capital_expenditure\"].iloc[-4:].std()\n",
    "        company_data[\"earnings_std\"] = company_data['earnings_per_basic_share_usd'].iloc[-4:].std()\n",
    "        company_data[\"earnings_std\"] = company_data[\"earnings_std\"].replace(np.inf, np.nan)\n",
    "        company_data[\"cfo_std\"] = company_data['net_cash_flow_from_operations'].iloc[-4:].std()\n",
    "        company_data[\"cfo_std\"] = company_data[\"cfo_std\"].replace(np.inf, np.nan)\n",
    "\n",
    "        company_data[\"avg_assets\"] = (company_data[\"total_assets\"] + company_data[\"total_assets\"].shift(1)) / 2\n",
    "        company_data[\"avg_equity\"] = (company_data[\"shareholders_equity\"] + company_data[\"shareholders_equity\"].shift(1)) / 2\n",
    "        company_data[\"avg_wc\"] = (company_data[\"working_capital\"] + company_data[\"working_capital\"].shift(1)) / 2\n",
    "        company_data[\"avg_debt\"] = (company_data[\"total_debt\"] + company_data[\"total_debt\"].shift(1)) / 2\n",
    "        \n",
    "        # setup for targets later\n",
    "        company_data[\"fwd_fcf_per_share\"] = company_data[\"free_cash_flow_per_share\"].shift(-1)\n",
    "#         company_data[\"fwd_eps\"] = company_data[\"earnings_per_basic_share_usd\"].shift(-1)\n",
    "\n",
    "        # replace with 0 -- it is likely four straight 0 periods\n",
    "        company_data[\"capex_std\"] = company_data[\"capex_std\"].replace(np.inf, 0.0)\n",
    "\n",
    "        # for calculations that resulted in Nans, impute with median. should this be done later?\n",
    "        company_data[\"capex_std\"] = company_data[\"capex_std\"].fillna(company_data[\"capex_std\"].median())\n",
    "        company_data[\"earnings_std\"] = company_data[\"capex_std\"].fillna(company_data[\"earnings_std\"].median())\n",
    "        company_data[\"avg_assets\"] = company_data[\"avg_assets\"].fillna(company_data[\"avg_assets\"].median())\n",
    "        company_data[\"avg_equity\"] = company_data[\"avg_equity\"].fillna(company_data[\"avg_equity\"].median())\n",
    "        company_data[\"avg_wc\"] = company_data[\"avg_wc\"].fillna(company_data[\"avg_wc\"].median())\n",
    "        company_data[\"avg_debt\"] = company_data[\"avg_debt\"].fillna(company_data[\"avg_debt\"].median())\n",
    "        company_data[\"avg_capital\"] = company_data[\"avg_debt\"] + company_data[\"avg_equity\"]\n",
    "\n",
    "        # get year, month, day\n",
    "        company_data[\"year\"] = company_data[\"date_dt\"].dt.year\n",
    "        company_data[\"month\"] = company_data[\"date_dt\"].dt.month\n",
    "        company_data[\"day\"] = company_data[\"date_dt\"].dt.day\n",
    "\n",
    "        # convert to strings for ID\n",
    "        company_data[\"year\"] = company_data[\"year\"].astype(str)\n",
    "        company_data[\"month\"] = company_data[\"month\"].astype(str)\n",
    "        company_data[\"day\"] = company_data[\"day\"].astype(str)\n",
    "\n",
    "        # get ID for merge\n",
    "        company_data[\"period\"] = company_data[\"year\"] + \"-\" + company_data[\"month\"]\n",
    "        company_data[\"concat\"] = company_data[\"ticker_symbol\"] + \"-\" + company_data[\"period\"]\n",
    "\n",
    "        merged_data_final = monthly_data.merge(company_data, left_on=\"id\", right_on=\"concat\", how=\"outer\")\n",
    "    \n",
    "#         macro_data = all_macro_data\n",
    "#         macro_data[\"ticker\"] = ticker\n",
    "#         macro_data[\"id\"] = macro_data[\"ticker\"] + \"-\" + macro_data[\"date\"]\n",
    "#         merged_data_final = merged_data.merge(macro_data, left_on=\"id\", right_on=\"id\", how=\"outer\")\n",
    "\n",
    "        # just ffill as we only need quarterly values and this will avoid look-ahead bias\n",
    "#         for c in macro_columns:\n",
    "#             merged_data_final[c] = merged_data_final[c].fillna(method=\"ffill\")\n",
    "\n",
    "        equity_data[ticker] = merged_data_final.dropna(subset=[\"volatility\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probably merge macro data after everything else is done\n",
    "equity_df = pd.concat([equity_data[t] for t in equity_data])\n",
    "equity_df = equity_df.dropna(subset=[\"concat\"])\n",
    "equity_df[\"quarter\"] = equity_df[\"date_dt_x\"].dt.quarter\n",
    "equity_df[\"quarter_end\"] = equity_df[\"quarter\"] % 3\n",
    "equity_df = equity_df.merge(sharadar_sectors[[\"ticker\", \"sicsector\"]], left_on=\"ticker_symbol\", right_on=\"ticker\")\n",
    "\n",
    "# drop any duplicates\n",
    "equity_df = equity_df[~equity_df[\"id\"].duplicated()]\n",
    "\n",
    "equity_df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to CSV\n",
    "equity_df.to_csv(\"C:/Users/lbianculli/dev/us_equities/ML_categorization/all_fundamentals.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final_preprocessing file next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
